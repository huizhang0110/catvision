
# CatVision

[**ğŸ¤— models**](https://huggingface.co/huizhang0110/CatVision)

## Introduction

A multimodal large-scale model, characterized by its open-source nature, closely emulates the functionalities of the GPT4V/Qwen-VL-Plus model. Built upon the foundation of Qwen-72b-Chat, CatVision in handling inputs that combine both images and text. This model is designed to effectively follow instructions for output formats, benefiting from the strengths of Qwen72b. 

ä¸€ä¸ªå¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œç´§å¯†æ¨¡æ‹Ÿäº†GPT4V/Qwen-VL-PLUSç³»åˆ—æ¨¡å‹çš„åŠŸèƒ½ã€‚è¯¥æ¨¡å‹å»ºç«‹åœ¨Qwen-72b-Chatçš„åŸºç¡€ä¸Šï¼Œå¯ä»¥å¤„ç†åŒ…å«äº¤é”™çš„å›¾æ–‡è¾“å…¥ã€‚è¯¥æ¨¡å‹ä»Qwen72bçš„ä¼˜åŠ¿ä¸­å—ç›Šï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°éµå¾ªè¾“å‡ºæ ¼å¼æŒ‡ä»¤ã€‚

Our model performs close to the closed-source Qwen-VL-PLUS on many datasets and significantly surpasses the performance of the open-source model Qwen-VL-7B-Chat.

æˆ‘ä»¬çš„æ¨¡å‹åœ¨å¾ˆå¤šæ•°æ®é›†ä¸Šï¼Œæ¥è¿‘é—­æºçš„Qwen-VL-PLUSçš„æ•ˆæœï¼Œå¹¶å¤§å¹…è¶…è¿‡å¼€æºæ¨¡å‹Qwen-VL-7B-Chatçš„æ•ˆæœã€‚

- Our training approach consisted of two stages, inspired by LLava1.5. In the initial stage, we trained the visual encoder + perceptual resampler, and in the second stage, we focused on training the large language model + perceptual resampler with instructional data. To overcome limited computational resources (32xA100-80G), we used Lora for training in both stages.

å—LLava1.5å¯å‘ï¼Œæˆ‘ä»¬çš„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šåœ¨åˆå§‹é˜¶æ®µï¼Œæˆ‘ä»¬è®­ç»ƒäº†è§†è§‰ç¼–ç å™¨+æ„ŸçŸ¥é‡é‡‡æ ·å™¨ï¼›åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä¸“æ³¨äºä½¿ç”¨è§†è§‰æŒ‡ä»¤æ•°æ®è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹+æ„ŸçŸ¥é‡é‡‡æ ·å™¨ã€‚ä¸ºäº†å…‹æœæœ‰é™çš„è®¡ç®—èµ„æº(32xA100-80G)ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªé˜¶æ®µéƒ½ä½¿ç”¨äº†Loraè¿›è¡ŒåŸ¹è®­ã€‚

- During the first stage, our training data included samples from ShareGPT4V and CC12M. As we progressed to the second stage, our training dataset encompassed ShareGPT4V fine-tune data, LVIS Instruct4V, OCR data, InforGraphics/Chart QA data, and data sourced from region descriptions in VG.

åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®åŒ…æ‹¬æ¥è‡ªShareGPT4Vå’ŒCC12Mçš„æ ·æœ¬ã€‚ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†åŒ…æ‹¬ShareGPT4Vå¾®è°ƒæ•°æ®ã€LVIS Instruct4Vã€OCRæ•°æ®ã€ä¿¡æ¯å›¾è¡¨é—®ç­”æ•°æ®ä»¥åŠä»VGåŒºåŸŸæè¿°ä¸­è·å–çš„æ•°æ®ã€‚

- The visual encoding part is inherited from Qwen-VL-Chat, i.e., Openclip ViT-bigG.

è§†è§‰ç¼–ç éƒ¨åˆ†ç»§æ‰¿è‡ªQwen-VL-Chatï¼Œå³Openclip ViT-bigGã€‚

- We are continuously collecting instruction data, optimizing the model, and looking forward to supporting more tasks.

æˆ‘ä»¬æ­£åœ¨æŒç»­æ”¶é›†æŒ‡ä»¤æ•°æ®ï¼Œä¼˜åŒ–æ¨¡å‹ï¼ŒæœŸå¾…èƒ½æ”¯æŒæ›´å¤šçš„åŠŸèƒ½ã€‚

## Quick Start

```
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
tokenizer = AutoTokenizer.from_pretrained(
    pretrained_model_name_or_path="huizhang0110/CatVision",
    model_max_length=8192,
    padding_side="left",
    trust_remote_code=True
)
config = AutoConfig.from_pretrained(
    pretrained_model_name_or_path="huizhang0110/CatVision",
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path="huizhang0110/CatVision",
    config=config,
    device_map="auto", 
    trust_remote_code=True
).eval()
query = "<img>demo.jpg</img>\nä»‹ç»ä¸€ä¸‹è¿™å¼ å›¾åƒï¼"
response, history = model.chat(
    tokenizer,
    query=query,
    history=None,
)
```

## Benchmark

Our model achieved favorable results on the many leaderboards.

- **[MMMU](https://eval.ai/web/challenges/challenge-page/2179/leaderboard/5377)**

| Model                          | Val (900) | Test (11K)   |
|--------------------------------|:---------:|:------------:|
| Gemini Ultra                   |   59.4    |     ----     |
| GPT4V                          |   56.8    |     55.7     |
| Gemini Pro                     |   47.9    |     ----     |
| Yi-VL-34B                      |   45.9    |     41.6     |
| Qwen-VL-PLUS                   |   45.2    |     40.8     |
| **CatVision**                  |   45.9    |     40.1     |
| Macro-VL                       |   41.2    |     40.4     |
| InfiMM-Zephyr-7B                |   39.4    |     35.5     |
| Yi-VL-6B                       |   39.1    |     37.8     |
| SVIT                           |   38.0    |     34.1     |
| LLaVA-1.5-13B                  |   36.4    |     33.6     |
| Emu2-Chat                      |   36.3    |     34.1     |
| Qwen-VL-7B-Chat                |   35.9    |     32.9     |

- **[CMMMU](https://github.com/CMMMU-Benchmark/CMMMU/blob/main/README.md)**

| Model                          | Val (900) | Test (11K)   |
|--------------------------------|:---------:|:------------:|
| GPT-4V(ision) (Playground)     |   42.5    |     43.7     |
| Qwen-VL-PLUS*                  |   39.5    |     36.8     |
| **CatVision**                  |   39.6    |     ----     |
| Yi-VL-34B                      |   36.2    |     36.5     |
| Yi-VL-6B                       |   35.8    |     35.0     |
| Qwen-VL-7B-Chat                |   30.7    |     31.3     |
| InternVL-Chat-ViT-6B-Vicuna-7B |   26.4    |     26.7     |
| InternVL-Chat-ViT-6B-Vicuna-13B|   27.4    |     26.1     |
| CogAgent-Chat                  |   24.6    |     23.6     |
| Emu2-Chat                      |   23.8    |     24.5     |
| Chinese-LLaVA                  |   25.5    |     23.4     |
| VisCPM                         |   25.2    |     22.7     |
| mPLUG-OWL2                     |   20.8    |     22.2     |
| Frequent Choice                |   24.1    |     26.0     |
| Random Choice                  |   21.6    |     21.6     |

- **[MMBench](https://mmbench.opencompass.org.cn/leaderboard)**

| Model               | mmbench_cn (test) | mmbench_cn (dev) | mmbench_en (test) | mmbench_zh (dev) | ccbench | 
|---------------------|:-----------------:|:----------------:|:-----------------:|:----------------:|:-------:|
| Qwen-VL-PLUS(BASE)  | 83.3              | 83.2             | 82.7              | 81.5             | 77.6    |
| GPT4v               | 77.0              | 75.1             | 74.4              | 75.0             | 46.5    |
| Qwen-VL-PLUS        | 67.0              | 66.2             | 70.7              | 69.6             | 55.1    |
| **CatVision**       | 70.9              | 71.8             | 70.2              | 71.6             | 49.8    |
| Qwen-VL-Chat        | 61.8              | 60.6             | 56.3              | 56.7             | 41.2    |

- **[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)**

| Model         | Perception | Cognition | 
|---------------|:----------:|:---------:|
| GPT4v         | 1409.43    | 517.14    |
| Qwen-VL-PLUS  | 1681.25    | 502.14    |
| **CatVision** | 1560.90    | 366.43    |
| Qwen-VL-Chat  | 1487.57    | 360.71    |

- **Open Compress**

wait

- **Show Case**

*å›¾åƒæè¿°*

![å›¾åƒæè¿°](image.png)

*ä¿¡æ¯å›¾è¡¨*

![å›¾åƒé—®ç­”](image-1.png)

*åŒºåŸŸç†è§£*

![åŒºåŸŸç†è§£](image-2.png)

## Citation 

```
@misc{CatVision,
  author = {zhanghui@4paradigm.com},
  title = {CatVision},
  year = {2024},
  publisher = {huggingface},
  howpublished = {\url{https://huggingface.co/huizhang0110/CatVision}}
}
```

![logo](./catvision.png)

